---
applyTo: '**'
---

<todos title="ZAP AI Traffic Analyst - Phase 3 (Release polish + upstream notes)" agentRequirement="Review steps frequently throughout the conversation and DO NOT stop between steps unless they explicitly require it. Keep the todos updated using the todo_write tool (do not edit this file).">
- [x] bump-version-and-manifest: Bump add-on version/semver and ensure manifest template/description matches LLM add-on integration and ZAP >= 2.15.0 requirement. ðŸ”´
  _Version set to 1.2.0; template updated; generated manifest verified._
- [x] update-changelog: Update CHANGELOG.md for the release: document switch to LLM add-on, removal of Ollama, and minimum ZAP version. ðŸ”´
  _Added 1.2.0 entry dated 2026-02-01 with Added/Changed/Removed._
- [x] clean-ollama-mentions: Remove stale Ollama references from user-facing docs/i18n strings that no longer apply. ðŸŸ¡
  _Removed Ollama-related user-facing strings; remaining mentions (if any) are internal-only._
- [x] add-upstream-llm-notes: Add a short doc describing upstream LLM add-on enhancements that would benefit this add-on (structured prompts, response-format control, streaming). ðŸŸ¢
  _Captured actionable upstream ideas in docs/upstream-llm-notes.md._
- [x] verify-build-and-zap-bundle: Run ./gradlew clean test jarZapAddOn and confirm generated ZapAddOn.xml still includes llm dependency + semver. ðŸ”´
  _Build successful; generated manifest includes llm dependency + semver; bundle produced under build/zapAddOn/bin._
</todos>

Phase 0: Decide integration mode (in progress)
Choose between (A) hard dependency on ZAP llm add-on vs (B) optional integration with Ollama fallback. Define minimum ZAP and llm add-on versions; confirm streaming is not required in Phase 1 (upstream llm is blocking today).
Phase 1: Add llm dependencies
Build: add compile dependency on llm artifact when available (likely org.zaproxy.addon:llm:<version>). Runtime: ensure generated ZapAddOn.xml declares <dependencies><addon>llm</addon></dependencies> via zapAddOn DSL or generateZapAddOnManifest post-processing in build.gradle.kts.
Phase 1: Introduce adapter layer
Create internal interface (e.g., AnalystLlmClient) used by the rest of the add-on. Implement LlmAddonClient backed by ExtensionLlm/LlmCommunicationService. If using fallback mode, also implement OllamaFallbackClient.
Phase 1: Refactor call sites to adapter
Replace direct OllamaClient usage in ExtensionAiAnalyst (custom analysis) and AnalystPopupMenu (GET/POST flows) with calls to AnalystLlmClient. Keep executor/background threads and EDT UI updates unchanged.
Phase 1: Prompt mapping decision
Choose between service.chat(String) with concatenated prompt vs service.chat(ChatRequest) with explicit system/user messages. Ensure outputs remain Markdown-friendly even for providers configured for JSON. Keep prompt-injection guard and truncation logic.
Phase 1: User-facing behavior
When llm add-on is missing or not configured, show clear guidance in the Analyst panel (use ExtensionLlm.isConfigured() and getCommsIssue()). Update README.md to explain configuration now lives in the official LLM add-on.
Phase 1: Tests and verification
Update/add tests to mock the adapter rather than Ollama HTTP. Validate ./gradlew test jarZapAddOn and verify in ZAP UI that analysis still works and failure states are friendly.
Phase 2: Remove legacy Ollama config UI
Remove or hide Ollama URL/model selection from AnalystOptionsPanel and deprecate/remove aitrafficanalyst.ollama.* keys from AnalystOptions. If fallback mode is retained, keep settings but mark legacy and only use when llm add-on unavailable.
Phase 2: Remove OllamaClient + dependencies (if no fallback)
If hard llm dependency is chosen, delete OllamaClient and remove OkHttp dependency and model-refresh logic. Ensure build stays clean and no dead code paths remain.
Phase 3: Upstream improvements + release
Propose/PR upstream llm improvements needed for advanced add-ons (streaming + structured prompts, provider response-format configuration) and then complete release work: bump version, update CHANGELOG.md, confirm manifest constraints, build + smoke-test final .zap before publishing. If useful, add a one-time migration helper that offers to create an llm provider config from existing aitrafficanalyst.ollama.* settings.